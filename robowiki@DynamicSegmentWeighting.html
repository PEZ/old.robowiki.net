<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML><HEAD><TITLE>DynamicSegmentWeighting - Robo Wiki -= Collecting Robocode Knowledge =-</TITLE>
<META NAME='KEYWORDS' CONTENT='Dynamic, Segment, Weighting'/>
<LINK REL="stylesheet" HREF="/robodocs/wiki.css">
</HEAD><BODY BGCOLOR="white">
<div class=wikiheader><h1><a href="robowiki@Robo_Home"><img src="/images/RoboWiki.png" alt="[Home]" border=0 align="right"></a><a href="robowiki@back=DynamicSegmentWeighting">DynamicSegmentWeighting</a></h1><a href="robowiki@Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki@Changes" class=wikipagelink>Changes</a> | <a href="robowiki@action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<hr class=wikilineheader></div><div class=wikitext>Something that is currently being implemented in <a href="robowiki@DrussGT" class=wikipagelink>DrussGT</a>.... it should make my surfing ignore all those buffers with strange segment types that don't help... ;-)
<p>
After I get some tests done I'll write up a full description. Including whether it helps or not =) -- <a href="robowiki@Skilgannon" class=wikipagelink>Skilgannon</a>
<p>
Oooh, interesting. Does this do it on the level of how buffers are merged, or on the level of changing the actual context of the buffers? If the former, this could be extremely similar to my dynamically weighted <a href="robowiki@CrowdTargeting" class=wikipagelink>CrowdTargeting</a> that I'm currently trying to improve :) -- <a href="robowiki@Rednaxela" class=wikipagelink>Rednaxela</a>
<p>
Well, first attempts don't seem to do much. Basically what I'm doing is, when logging hits, seeing which sets of bins (extracted from the segments) predicted that hit correctly. The ones that predict right get weighted higher. The important part is weighting *that* set of bins at *that* location in the buffer, so that a different location in the same buffer doesn't benefit from the weighting. Applying this to <a href="robowiki@DynamicClustering" class=wikipagelink>DynamicClustering</a> would be a bit harder though. Perhaps keeping track of the shots that make up a wave, and increasing the 'importance' of the ones that result in a successful hit?
<p>
However, one problem I now see with this is a constant shuffling of buffers because *if* it gets a hit, it is due to the fact that currently high-weighted buffers didn't predict it, whether or not they happen to be the best. Maybe it would be better to only use this on 'unbiased' buffer additions, ie. bullet-hit-bullet hits. Giving that a try now =) -- <a href="robowiki@Skilgannon" class=wikipagelink>Skilgannon</a>
</div><hr class=wikilinefooter>
<div class=wikifooter><form method="post" action="robowiki" enctype="application/x-www-form-urlencoded">
<a href="robowiki@Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki@Changes" class=wikipagelink>Changes</a> | <a href="robowiki@action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<a href="robowiki@action=edit&id=DynamicSegmentWeighting" class=wikipageedit>Edit text of this page</a> | <a href="robowiki@action=history&id=DynamicSegmentWeighting">View other revisions</a><br>Last edited June 2, 2008 18:36 EST by <a href="robowiki@Skilgannon" title="ID 14128 from wblv-ip-pcache-4-vif1.telkom-ipnet.co.za">Skilgannon</a> <a href="robowiki@action=browse&diff=1&id=DynamicSegmentWeighting">(diff)</a><br>Search: <input type="text" name="search"  size="20" /><input type="hidden" name="dosearch" value="1"  /></form></div>
</body>
</html>