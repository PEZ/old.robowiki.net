<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML><HEAD><TITLE>Segmentation/Implemenation - Robo Wiki -= Collecting Robocode Knowledge =-</TITLE>
<META NAME='KEYWORDS' CONTENT='Segmentation, Implemenation'/>
<LINK REL="stylesheet" HREF="/robodocs/wiki.css">
</HEAD><BODY BGCOLOR="white">
<div class=wikiheader><h1><a href="robowiki?Robo_Home"><img src="/images/RoboWiki.png" alt="[Home]" border=0 align="right"></a><a href="robowiki?back=/Implemenation">Segmentation/Implemenation</a></h1><a href="robowiki?Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki?Segmentation" class=wikipagelink>Segmentation</a> | <a href="robowiki?Changes" class=wikipagelink>Changes</a> | <a href="robowiki?action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<hr class=wikilineheader></div><div class=wikitext><H2>Implementation</H2>

<p>
This is probably the most important part of segmentation. Once the segments are selected, you need to put them in action.  As you fire waves and gather information, you add them to the appropriate part of each segment.  Logically, the more segments you have and the finer each segment is divided, the more exact you can make your predictions. The downside is that the greater the granularity, the less data you have to make good decisions.  So the goal is to maintain the balance between granularity and population. Since the population is constantly increasing, this is the perfect situation to have a dynamic system to balance the two.  Ideally, segmentation should begin mostly unsegmented, and then increase to an incredibly fine view as time progresses.  
<p>
There are two ways to accomplish this. One is to add values to several arrays and then select the array with the best balance. The other is to have a single array of all wanted segments, but starting with a very low division within each segment.  These divisions then increase as more data is gained based on what is most effective.
<p>
The multiple array approach was brought forth in <a href="robowiki?Fractal" class=wikipagelink>Fractal</a>'s <a href="robowiki?AutomatedSegmentation" class=wikipagelink>AutomatedSegmentation</a> approach, which serves to be quite fast and very straightforward.  
<p>
The <a href="robowiki?WikiTargeting/DynamicSegmentation" class=wikipagelink>WikiTargeting/DynamicSegmentation</a> tree approach gives one of the most specific implementations of the granularity method, with processing time being the only big factor.
<p>
Now the biggest challenge, and probably the most obscure part of an effective segmentation design, is to decide the best level of segmentation for the data collected.  The goal is to find the balance between population and granularity, while trying to get the spikiest profile possible.  The granularity is most valuable to be maximized, but there are some situations where the granularity reaches a point where it no longer sacrifices population.  For example, at a certain distance, once the granularity for the visit counts is smaller than the width of the robot, using a window smoothing technique you can make the segments as small as you want without losing population.  Population is a very tweakable concept, to find a balance to maintain data while still being able to get as much granularity as possible, probably to select a goal average to reach.
<p>
-- <a href="robowiki?Jokester" class=wikipagelink>Jokester</a>
<p>
<H3>Keep it Simple and Stupid</H3>

I am one who has done a <strong>lot</strong> of experimenting with this trying to increasingly use more granularity as more data is collected. But I have to say nothing works as well (for me) as simply using the same granularity all the time. I have no idea why this is so, but it is. And my guns are among the very strongest developed yet. See <a href="robowiki?TargetingChallenge/Results" class=wikipagelink>TargetingChallenge/Results</a> for some proof of this (where all my best guns are the strongest in each bot weight class, <a href="robowiki?MegaBot" class=wikipagelink>MegaBot</a>, <a href="robowiki?MiniBot" class=wikipagelink>MiniBot</a> and <a href="robowiki?MicroBot" class=wikipagelink>MicroBot</a>).
<p>
My guns lack any kind of DynamicSegmentation<a href="robowiki?action=edit&id=DynamicSegmentation" class=wikipageedit>?</a> or <a href="robowiki?BinSmoothing" class=wikipagelink>BinSmoothing</a> or <a href="robowiki?MultipleChoice" class=wikipagelink>MultipleChoice</a> or any such "advanced" concept. Maybe it is just simplicity that gives them the edge, I might have one of the few quite bug-free implementations. But maybe it just is that all that mumbo-jumbo isn't needed. What you need is collect raw data on carefully selected segmentations and then follow the orders you get from that data <strong>without questioning</strong>. Blind Faith if you will. Stupid Blind Faith even. The <a href="robowiki?RoboRumble" class=wikipagelink>RoboRumble</a> environment is more complex than you think. Your guesses have a high probability of being wrong.
<p>
In any case bug-freeness is never wrong. Even when the bugs increase your bot's performance they should be hunted down. They keep you from reaching for the limits of your idea. Keeping it <strong>as simple as possible</strong> is certainly one way to go about bug-freeness.
<p>
-- <a href="robowiki?PEZ" class=wikipagelink>PEZ</a>
<p>
Now that is very interesting.  I am currently doing my tests using a small variation of the <a href="robowiki?GuessFactorTargeting/Tutorial" class=wikipagelink>GuessFactorTargeting/Tutorial</a> gun, to which I have created a pluggable segmentation scheme.  I havent done many tests yet, but I will like to see how results differ between unsegmented, stationarily segmented, and high level dynamic segmentation.  As it appears from your description, it seems that hand crafting is very effective.  But then I have a bit of a theory that the best guns vs the best movements boil down to just the best <a href="robowiki?RandomTargeting" class=wikipagelink>RandomTargeting</a> and flattening there is.  What makes movements and targeting stand out, are how they deal with the lesser bots, not the top ones.  That is where <a href="robowiki?WaveSurfing" class=wikipagelink>WaveSurfing</a> made its "splash" (pardon the pun), in that it allowed for the best scores possible against the basic bots.  This is also where I feel good targeting is going, not so much to beat the best movements, but to be perfect against those not so great.  I remember the first time I put my perfected circular targeting against spin bot, and watched me get 100% accuracy (when it wasnt hitting a wall).  I feel that that is the goal of modern targeting, to make a method that will hit walls, spinbot, and all the lesser pattern bots and spiky movers with as great of accuracy as possible.  I believe that that is where a good part of your success is coming from, in that you have a very perfectly tuned system.  I just wonder how much it could be improved upon.  I assume that after 200 rounds or so your gun is far more accurate than it was in the first 10, but how does the accuracy of those early rounds compare to the accuracy of a less or differently segmented gun...  Who knows, this is probably all for naught, but I am not quite sure why. -- <a href="robowiki?Jokester" class=wikipagelink>Jokester</a>
<p>
Any general enough and learning gun should be far more accurate after 200 rounds than it is in the first 10 rounds. My guns are tuned for the <a href="robowiki?RoboRumble" class=wikipagelink>RoboRumble</a>@Home mostly and that means I want it to be better than other guns in 35 round battles. <a href="robowiki?TargetingChallenge/ResultsFastLearning" class=wikipagelink>TargetingChallenge/ResultsFastLearning</a> measures that somewhat better than the 500 rounds version. I call this TC35 and my guns are maybe even stronger there, relatively speaking. Where my gun fails is against <a href="robowiki?WaveSurfers" class=wikipagelink>WaveSurfers</a> really. Because my gun is the strongest against weak movement. Think about it, even with quite high segmentation weak movements reveal themselves quite fast. If your segemenation is high enough you only need one sample in the most visited segments against really weak movements. High segmentation also helps the most against strong movement. Anyway, <a href="robowiki?CassiusClay/Bee" class=wikipagelink>Bee</a> actually has a lesser segmented visit counts array which is trying to catch up a bit faster. What I do is I simply overlay the high and low segmented arrays when choosing where to fire. Careful tuning on the segmentation levels here makes quite a big difference. I can also confess I do some <a href="robowiki?BinSmoothing" class=wikipagelink>BinSmoothing</a>. But it doesn't really make a difference in performance, and I <strong>have</strong> tested it. Extensively. It just "feels" better to do it than to skip it. (Since it doesn't degrade performance either.) -- <a href="robowiki?PEZ" class=wikipagelink>PEZ</a>
</div><hr class=wikilinefooter>
<div class=wikifooter><form method="post" action="robowiki" enctype="application/x-www-form-urlencoded">
<a href="robowiki?Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki?Segmentation" class=wikipagelink>Segmentation</a> | <a href="robowiki?Changes" class=wikipagelink>Changes</a> | <a href="robowiki?action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<a href="robowiki?action=edit&id=Segmentation/Implemenation" class=wikipageedit>Edit text of this page</a> | <a href="robowiki?action=history&id=Segmentation/Implemenation">View other revisions</a><br>Last edited June 28, 2005 0:35 EST by <a href="robowiki?PEZ" title="ID 3989 from 1-1-7-24a.dre.sth.bostream.se">PEZ</a> <a href="robowiki?action=browse&diff=1&id=Segmentation/Implemenation">(diff)</a><br>Search: <input type="text" name="search"  size="20" /><input type="hidden" name="dosearch" value="1"  /></form></div>
</body>
</html>