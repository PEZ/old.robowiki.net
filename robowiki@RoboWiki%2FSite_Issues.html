<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML><HEAD><TITLE>RoboWiki/Site Issues - Robo Wiki -= Collecting Robocode Knowledge =-</TITLE>
<META NAME='KEYWORDS' CONTENT='Robo, Wiki, Site, Issues'/>
<LINK REL="stylesheet" HREF="/robodocs/wiki.css">
</HEAD><BODY BGCOLOR="white">
<div class=wikiheader><h1><a href="robowiki@Robo_Home"><img src="/images/RoboWiki.png" alt="[Home]" border=0 align="right"></a><a href="robowiki@back=/Site+Issues">RoboWiki/Site Issues</a></h1><a href="robowiki@Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki@RoboWiki" class=wikipagelink>RoboWiki</a> | <a href="robowiki@Changes" class=wikipagelink>Changes</a> | <a href="robowiki@action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<hr class=wikilineheader></div><div class=wikitext>Dudes. I don't know if you have noticed it. But the latest weeks the wiki has been under serious attack from wiki spammers. They try to replace page content with lots of links to other sites. Trying to boost Google precense of those sites I guess. Anyway, I think it's something like 400 attempts the last two weeks. It's a huge increase, even considering the attacks earlier this year.
<p>
Those previous attacks made me patch the wiki script in a way that it doesn't allow posting of pages with more than just a few external links on them. It works. We would have 400 spammed pages without this "filter". Just letting you know. It could be good to know why some of your legitimate edits doesn't stick. Usually I add the attacker's IP-address to the banned list of addresses. Often I need to block entire sub nets. But these latest attacks are som many and from so many different sub nets that I think I happened to block people from our community. SO I have removed the ip-blocks entirely now. Means my link-filter is the only protection at the moment. Let's hope it's enough.
<p>
-- <a href="robowiki@PEZ" class=wikipagelink>PEZ</a>
<p>
Clever idea with the limit on the number of external links! -- <a href="robowiki@Pulsar" class=wikipagelink>Pulsar</a>
<p>
Don't you get trouble with the <a href="robowiki@RoboRumble/Participants" class=wikipagelink>RoboRumble/Participants</a> page if more and more robocoders do not use the <a href="robowiki@RobocodeRepository" class=wikipagelink>RobocodeRepository</a>, but use links to their own pages instead?  --<a href="robowiki@GrubbmGait" class=wikipagelink>GrubbmGait</a>
<p>
No, because the filter works like so it considers only the difference in number of external links. Means if you add a new extarnal link (or change one) it's OK. Or should be. You could verify it if you like. The real downside is that in order not to give the spam bots a clue of what's going on the filter is "silent". To a spam bot without the necessary checks it looks like it has succeeded. But it makes it a bit less user friendly for legit human users. -- <a href="robowiki@PEZ" class=wikipagelink>PEZ</a>
<p>
BTW. The idea with that filter isn't mine. The implementation is though. -- <a href="robowiki@PEZ" class=wikipagelink>PEZ</a>
<p>
Can't you measure the time between clicking "Edit text of this page" and the submit button? If it's under, say, 2 seconds, then it is either a spambot, or a very fast typer... -- <a href="robowiki@Dummy" class=wikipagelink>Dummy</a>
<p>
Possibly. But actually I don't know how to do it. And I would need to include exeptions for my own bots and any other bot posting legit content. -- <a href="robowiki@PEZ" class=wikipagelink>PEZ</a>
<p>
Also, what if your only changing a spelling error that happens to be in the top of the page? --<a href="robowiki@UnderDark" class=wikipagelink>UnderDark</a>
<p>
That shouldn't be a problem. -- ~~~~
<p>
Hi, i'm unable to edit <a href="robowiki@Ascendant" class=wikipagelink>Ascendant</a>s page. Obviously it works fine with other pages though. In case that filter described above is still in use: i was not adding external links. Is it just me or is there something wrong with that page? --<a href="robowiki@Mue" class=wikipagelink>mue</a>
<p>
I just tried to edit <a href="robowiki@Ascendant" class=wikipagelink>Ascendant</a>s page and it didn't work either. -- <a href="robowiki@Florent" class=wikipagelink>Florent</a>
<p>
OK. I guess I tightened it up a bit too much. We're (<a href="robowiki@Voidious" class=wikipagelink>voidious</a> and I) are looking into a smarter solution. Meanwhile I have loosened up the restrictions again so that <a href="robowiki@Ascendant" class=wikipagelink>Ascendant</a> and other pages containing URLs will be editable again. It means our <a href="robowiki@RevertingVandalism" class=wikipagelink>RevertingVandalism</a> efforts will have to be taken into account again. -- <a href="robowiki@PEZ" class=wikipagelink>PEZ</a>
<p>
I have no idea how the wiki script works, but can we force people to sign in before editing any page? -- <a href="robowiki@Florent" class=wikipagelink>Florent</a>
<p>
That part of the script is quite strange. It was actually what I thought I did with the last tightening-up of things. But obviously I hadn't understood things right. -- <a href="robowiki@PEZ" class=wikipagelink>PEZ</a>
<p>
Thank you, its working again. --<a href="robowiki@Mue" class=wikipagelink>mue</a>
</div><hr class=wikilinefooter>
<div class=wikifooter><form method="post" action="robowiki" enctype="application/x-www-form-urlencoded">
<a href="robowiki@Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki@RoboWiki" class=wikipagelink>RoboWiki</a> | <a href="robowiki@Changes" class=wikipagelink>Changes</a> | <a href="robowiki@action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<a href="robowiki@action=edit&id=RoboWiki/Site_Issues" class=wikipageedit>Edit text of this page</a> | <a href="robowiki@action=history&id=RoboWiki/Site_Issues">View other revisions</a><br>Last edited May 7, 2006 16:07 EST by <a href="robowiki@Mue" title="ID 3525 from p54BCDAEB.dip.t-dialin.net">Mue</a> <a href="robowiki@action=browse&diff=1&id=RoboWiki/Site_Issues">(diff)</a><br>Search: <input type="text" name="search"  size="20" /><input type="hidden" name="dosearch" value="1"  /></form></div>
</body>
</html>