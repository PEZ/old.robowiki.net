<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML><HEAD><TITLE>RoboWiki/IPFiltering - Robo Wiki -= Collecting Robocode Knowledge =-</TITLE>
<META NAME='KEYWORDS' CONTENT='Robo, Wiki, IPFiltering'/>
<LINK REL="stylesheet" HREF="/robodocs/wiki.css">
</HEAD><BODY BGCOLOR="white">
<div class=wikiheader><h1><a href="robowiki?Robo_Home"><img src="/images/RoboWiki.png" alt="[Home]" border=0 align="right"></a><a href="robowiki?back=/IPFiltering">RoboWiki/IPFiltering</a></h1><a href="robowiki?Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki?RoboWiki" class=wikipagelink>RoboWiki</a> | <a href="robowiki?Changes" class=wikipagelink>Changes</a> | <a href="robowiki?action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<hr class=wikilineheader></div><div class=wikitext><H4>On the topic of blocking morons and vandals</H4>

You can make two rule files, normalrules:
<PRE >
 add 100 allow ip from any to any via lo*
 etc. Make sure rule 1000 is reached before anything is allowed.
</PRE>
and blockedips:
<PRE >
 delete 1000
 add 1000 deny ip from 12.34.56.78 to any
 add 1000 deny ip from 12.34.43.21 to any
 add 1000 deny ip from 65.43.21.0/24 to any
</PRE>
/24 is the amount of 'fixed' bits in a range, resulting in 65.43.21.0-65.43.21.255.<br>You can do this in a startup script:
<PRE >
 ipfw -q normalrulesfile
 ipfw -q blockedipsfile
</PRE>
and this to refresh the blocked IPs, after you updated the file:
<PRE >
 ipfw blockedipsfile
</PRE>
That should be manageable. -- <a href="robowiki?Jonathan" class=wikipagelink>Jonathan</a>
<p>
Thanks. Though the problem often is about finding out from what ip the vandalism came. I have sometimes blocked out good <a href="robowiki?RoboWiki" class=wikipagelink>RoboWiki</a> citizens when I have misinterpreted the logs. We'll have to watch the development on the vandalism issue. Lately their have been some idiot(s?) trying to post links to offensive sites hidden in the text of the pages. Most of those un-asked for advertisements have been detected and deleted I think. But if it gets too much of it we'll have to think up some antidote of course. I would really hate having to turn the robowiki into a moderated forum like some other wikis have been forced to do. -- <a href="robowiki?PEZ" class=wikipagelink>PEZ</a>
<p>
It is possible to save IP-addresses for every revision, besides WikiName<a href="robowiki?action=edit&id=WikiName" class=wikipageedit>?</a>s, but of course they should be only visible by you. Or maybe a WikiLog<a href="robowiki?action=edit&id=WikiLog" class=wikipageedit>?</a> in which you can easily look them up (you should be able to search for something like <em><a href="robowiki?RoboWiki/IPFiltering" class=wikipagelink>RoboWiki/IPFiltering</a> revision 2</em>). Anyone who can implement that? -- <a href="robowiki?Jonathan" class=wikipagelink>Jonathan</a>
<p>
Well, I would like the first option. I'll see if I can do that. -- <a href="robowiki?PEZ" class=wikipagelink>PEZ</a>
</div><hr class=wikilinefooter>
<div class=wikifooter><form method="post" action="robowiki" enctype="application/x-www-form-urlencoded">
<a href="robowiki?Robo_Home" class=wikipagelink>Robo Home</a> | <a href="robowiki?RoboWiki" class=wikipagelink>RoboWiki</a> | <a href="robowiki?Changes" class=wikipagelink>Changes</a> | <a href="robowiki?action=editprefs">Preferences</a> | <a href='?action=index'>AllPages</a><br>
<a href="robowiki?action=edit&id=RoboWiki/IPFiltering" class=wikipageedit>Edit text of this page</a> | <a href="robowiki?action=history&id=RoboWiki/IPFiltering">View other revisions</a><br>Last edited August 18, 2004 23:20 EST by <a href="robowiki?PEZ" title="ID 3032 from 192.168.0.xxx">PEZ</a> <a href="robowiki?action=browse&diff=1&id=RoboWiki/IPFiltering">(diff)</a><br>Search: <input type="text" name="search"  size="20" /><input type="hidden" name="dosearch" value="1"  /></form></div>
</body>
</html>